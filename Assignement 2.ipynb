{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Engineering Predictive Features\n",
    "\n",
    "**Student Name:** Sean Sampietro\n",
    "\n",
    "**Date:** 2/5/26\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you'll practice feature engineering by creating new predictive features from the Ames Housing dataset. You'll build a baseline model with raw features, engineer at least 5 new features based on real estate intuition, and measure how feature engineering improves model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKPOINT: Verify dataset loaded correctly\n",
      "Dataset shape: (1460, 81)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the Ames Housing dataset\n",
    "# TODO: Load train.csv from the data folder\n",
    "df = pd.read_csv(r\"C:\\Users\\Ssanp\\Downloads\\train.csv\")  # Replace with pd.read_csv()\n",
    "\n",
    "# Display basic information\n",
    "# TODO: Display the first few rows and basic info about the dataset\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT: Verify dataset loaded correctly\")\n",
    "print(f\"Dataset shape: {df.shape if df is not None else 'Not loaded'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Build Baseline Model with Raw Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Raw Features for Baseline\n",
    "\n",
    "Select 10-15 raw features to use in your baseline model. Here's a suggested starting set (you can adjust):\n",
    "\n",
    "**Suggested features:**\n",
    "- `GrLivArea` - Above grade living area square feet\n",
    "- `OverallQual` - Overall material and finish quality\n",
    "- `YearBuilt` - Original construction year\n",
    "- `TotalBsmtSF` - Total basement square feet\n",
    "- `FullBath` - Full bathrooms above grade\n",
    "- `BedroomAbvGr` - Bedrooms above grade\n",
    "- `GarageArea` - Size of garage in square feet\n",
    "- `LotArea` - Lot size in square feet\n",
    "- `Neighborhood` - Physical location (categorical)\n",
    "- Add 5-10 more features you think are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline features selected: 13\n",
      "Target variable shape: (1460,)\n"
     ]
    }
   ],
   "source": [
    "# Select features for baseline model\n",
    "# TODO: Create a list of feature names you want to use\n",
    "baseline_features = [\n",
    "  'OverallQual',\n",
    "    'GrLivArea',\n",
    "    'GarageCars',\n",
    "    'GarageArea',\n",
    "    'TotalBsmtSF',\n",
    "    '1stFlrSF',\n",
    "    'YearBuilt',\n",
    "    'YearRemodAdd',\n",
    "    'FullBath',\n",
    "    'TotRmsAbvGrd',\n",
    "    'Fireplaces',\n",
    "    'LotArea',\n",
    "    'Neighborhood'\n",
    "    # Add more features here\n",
    "]\n",
    "\n",
    "# TODO: Create X (features) and y (target) for baseline\n",
    "# Make sure to handle missing values and encode categorical variables\n",
    "X = df[baseline_features].copy()\n",
    "y = df['SalePrice']  # Replace with df['SalePrice']\n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "X[cat_cols] = X[cat_cols].fillna('None')\n",
    "\n",
    "print(f\"Baseline features selected: {len(baseline_features)}\")\n",
    "print(f\"Target variable shape: {y.shape if y is not None else 'Not defined'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKPOINT: After preprocessing\n",
      "X_baseline shape: (12, 1)\n",
      "Missing values: 0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "# TODO: Fill missing values appropriately\n",
    "# Numeric: Use median or 0\n",
    "# Categorical: Use 'None' or most frequent\n",
    "if isinstance(X_baseline, list):\n",
    "    X_baseline = pd.DataFrame(X_baseline)\n",
    "\n",
    "# Encode categorical variables\n",
    "# TODO: Use pd.get_dummies() for categorical features\n",
    "X_base = pd.get_dummies(X_base, drop_first=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT: After preprocessing\")\n",
    "print(f\"X_baseline shape: {X_baseline.shape if X_baseline is not None else 'Not defined'}\")\n",
    "print(f\"Missing values: {X_baseline.isnull().sum().sum() if X_baseline is not None else 'N/A'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12, 1460]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# TODO: Use train_test_split with test_size=0.2, random_state=42\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_baseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with train_test_split()\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train baseline Random Forest model\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# TODO: Create and train RandomForestRegressor(n_estimators=100, random_state=42)\u001b[39;00m\n\u001b[32m      7\u001b[39m baseline_model = RandomForestRegressor(\n\u001b[32m      8\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,\n\u001b[32m      9\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2916\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [12, 1460]"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "# TODO: Use train_test_split with test_size=0.2, random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_baseline, y, test_size=0.2, random_state=42)  # Replace with train_test_split()\n",
    "\n",
    "# Train baseline Random Forest model\n",
    "# TODO: Create and train RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "baseline_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "  # Replace with trained model\n",
    "\n",
    "# Make predictions\n",
    "# TODO: Generate predictions on test set\n",
    "baseline_predictions = baseline_model.predict(X_test)\n",
    "  # Replace with predictions\n",
    "\n",
    "# Calculate metrics\n",
    "# TODO: Calculate R² and RMSE\n",
    "baseline_r2 = r2_score(y_test, baseline_predictions)\n",
    "  # Replace with r2_score()\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_predictions))\n",
    "\n",
    "  # Replace with np.sqrt(mean_squared_error())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"R² Score: {baseline_r2 if baseline_r2 is not None else 'Not calculated'}\")\n",
    "print(f\"RMSE: ${baseline_rmse:,.2f}\" if baseline_rmse is not None else \"RMSE: Not calculated\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Baseline Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract and visualize feature importances\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# TODO: Get feature importances from baseline_model\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# TODO: Create a horizontal bar plot of top 10 features\u001b[39;00m\n\u001b[32m      4\u001b[39m rf_baseline = RandomForestRegressor(\n\u001b[32m      5\u001b[39m     n_estimators=\u001b[32m300\u001b[39m,\n\u001b[32m      6\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m rf_baseline.fit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[32m      9\u001b[39m baseline_importances = pd.Series(\n\u001b[32m     10\u001b[39m     rf_baseline.feature_importances_,\n\u001b[32m     11\u001b[39m     index=baseline_features\n\u001b[32m     12\u001b[39m ).sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     15\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract and visualize feature importances\n",
    "# TODO: Get feature importances from baseline_model\n",
    "# TODO: Create a horizontal bar plot of top 10 features\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "baseline_importances = pd.Series(\n",
    "    rf_baseline.feature_importances_,\n",
    "    index=baseline_features\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "baseline_importances.head(10).plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 10 Baseline Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKPOINT: Review which raw features are most important\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Engineer New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Quality x Area - [Interaction]\n",
    "\n",
    "**Business Justification:**\n",
    "This feature captures the interaction between a home’s overall quality and its living area. Larger homes tend to sell for higher prices, but size contributes more value when the home is also high quality. Real estate intuition suggests that buyers pay a premium for homes both spacious and well-built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your first engineered feature\n",
    "# Example: df['total_bathrooms'] = df['FullBath'] + 0.5 * df['HalfBath']\n",
    "df['Quality_x_Area'] = df['OverallQual'] * df['GrLivArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2: Home Age  - [Derived]\n",
    "\n",
    "**Business Justification:**\n",
    "This feature measures how old the home is at the time of sale. Newer homes generally command higher prices due to modern designs, updated systems, and lower expected maintenance costs. Home age is a strong proxy for depreciation in real estate markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your second engineered feature\n",
    "df['HomeAge'] = df['YrSold'] - df['YearBuilt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3: Has Fireplace - [Boolean]\n",
    "\n",
    "**Business Justification:**\n",
    "This feature indicates whether a home has at least one fireplace. Fireplaces are considered a luxury amenity and can increase buyer appeal, especially in colder climates. Buyers often value the presence of a fireplace more than the total number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your third engineered feature\n",
    "df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 4: [Overall Quality x Area] - [Quality]\n",
    "\n",
    "**Business Justification:**\n",
    "This feature measures the quality of a home relative to its size. It helps distinguish homes that are efficiently designed and high quality from large homes with lower construction quality. Buyers often pay more for homes that offer higher quality per square foot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your fourth engineered feature\n",
    "df['Qual_per_SF'] = df['OverallQual'] / (df['GrLivArea'] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 5: [Total Square Footage] - [Aggregation]\n",
    "\n",
    "**Business Justification:**\n",
    "This feature represents the total usable square footage of the home by combining basement and above-ground living space. Total living space is one of the most important drivers of housing prices. Larger homes generally command higher sale prices due to increased utility and comfort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your fifth engineered feature\n",
    "df['TotalSF'] = df['TotalBsmtSF'] + df['GrLivArea']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Engineered Features (Optional)\n",
    "\n",
    "You can create additional features beyond the required 5 if you think they'll improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create additional engineered features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Train Model with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Quality_x_Area', 'HomeAge', 'Qual_per_SF', 'TotalSF'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m all_features = baseline_features + engineered_features\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# TODO: Create X_engineered with all features\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Remember to handle missing values and encode categoricals\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_eng = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbaseline_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mengineered_features\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     17\u001b[39m X_engineered = df[engineered_features]\u001b[38;5;66;03m# Replace with your feature matrix\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal features in engineered model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Quality_x_Area', 'HomeAge', 'Qual_per_SF', 'TotalSF'] not in index\""
     ]
    }
   ],
   "source": [
    "# Create feature list combining baseline + engineered features\n",
    "# TODO: List all your engineered feature names\n",
    "engineered_features = [\n",
    "    'Quality_x_Area',\n",
    "    'HomeAge',\n",
    "    'Fireplaces',\n",
    "    'Qual_per_SF',\n",
    "    'TotalSF'   # Add your engineered feature names here\n",
    "]\n",
    "\n",
    "# Combine baseline and engineered features\n",
    "all_features = baseline_features + engineered_features\n",
    "\n",
    "# TODO: Create X_engineered with all features\n",
    "# Remember to handle missing values and encode categoricals\n",
    "X_eng = df[baseline_features + engineered_features].copy()\n",
    "X_engineered = df[engineered_features]# Replace with your feature matrix\n",
    "\n",
    "print(f\"Total features in engineered model: {len(all_features)}\")\n",
    "print(f\"New engineered features: {len(engineered_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split data (use same random_state for fair comparison)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# TODO: Split X_engineered and y\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_train_eng, X_test_eng, y_train_eng, y_test_eng = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_engineered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train model with engineered features\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# TODO: Train RandomForestRegressor(n_estimators=100, random_state=42)\u001b[39;00m\n\u001b[32m      7\u001b[39m engineered_model = RandomForestRegressor(\n\u001b[32m      8\u001b[39m     n_estimators= \u001b[32m100\u001b[39m,\n\u001b[32m      9\u001b[39m     random_state= \u001b[32m42\u001b[39m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2918\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m-> \u001b[39m\u001b[32m2918\u001b[39m n_samples = \u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:395\u001b[39m, in \u001b[36m_num_samples\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    393\u001b[39m         x = np.asarray(x)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x.shape) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: Expected sequence or array-like, got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Split data (use same random_state for fair comparison)\n",
    "# TODO: Split X_engineered and y\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model with engineered features\n",
    "# TODO: Train RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "engineered_model = RandomForestRegressor(\n",
    "    n_estimators= 100,\n",
    "    random_state= 42\n",
    ")\n",
    "  # Replace with trained model\n",
    "\n",
    "# Make predictions\n",
    "# TODO: Generate predictions on test set\n",
    "engineered_predictions = engineered_model.predict(X_test_eng)  # Replace with predictions\n",
    "\n",
    "# Calculate metrics\n",
    "# TODO: Calculate R² and RMSE\n",
    "engineered_r2 = r2_score(y_test_eng, engineered_predictions)  # Replace with r2_score()\n",
    "engineered_rmse = None  # Replace with np.sqrt(mean_squared_error())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENGINEERED MODEL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"R² Score: {engineered_r2 if engineered_r2 is not None else 'Not calculated'}\")\n",
    "print(f\"RMSE: ${engineered_rmse:,.2f}\" if engineered_rmse is not None else \"RMSE: Not calculated\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Compare Models and Identify Most Valuable Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "# TODO: Create a table comparing baseline vs engineered model\n",
    "comparison = None  # Replace with pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "# TODO: Display comparison table\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvement\n",
    "if baseline_r2 is not None and engineered_r2 is not None:\n",
    "    r2_improvement = ((engineered_r2 - baseline_r2) / baseline_r2) * 100\n",
    "    rmse_improvement = ((baseline_rmse - engineered_rmse) / baseline_rmse) * 100\n",
    "    print(f\"\\nR² Improvement: {r2_improvement:.2f}%\")\n",
    "    print(f\"RMSE Improvement: {rmse_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Importances from Engineered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize top 15 feature importances\n",
    "# TODO: Get feature importances from engineered_model\n",
    "# TODO: Create horizontal bar plot of top 15 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Most Valuable Features\n",
    "\n",
    "**Write 3-5 bullet points analyzing your results:**\n",
    "\n",
    "- [Which of YOUR engineered features appeared in the top 15 most important features?]\n",
    "- [Why do you think these specific features performed well?]\n",
    "- [Were any engineered features less valuable than you expected? Why?]\n",
    "- [What did you learn about feature engineering from this analysis?]\n",
    "- [If you were to create more features, what would you try based on these results?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Submit Your Work\n",
    "\n",
    "Before submitting:\n",
    "1. Make sure all code cells run without errors\n",
    "2. Verify you have at least 5 engineered features with business justifications\n",
    "3. Check that your comparison table and visualizations display correctly\n",
    "4. Complete the analysis section above\n",
    "\n",
    "Then push to GitHub:\n",
    "```bash\n",
    "git add .\n",
    "git commit -m 'completed feature engineering assignment'\n",
    "git push\n",
    "```\n",
    "\n",
    "Submit your GitHub repository link on the course platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
